# TF-IDF Tag Extractor

Простой инструмент командной строки для извлечения кандидатов в теги из корпуса статей на основе TF-IDF. Скрипт следует шагам, описанным в постановке задачи: предварительная очистка, замена важных сущностей на стабильные токены, построение n-грамм с TF-IDF и фильтрация «воды» и IoC.

## Возможности

- Очистка текста: понижение регистра, удаление Markdown/HTML, ссылок и лишней пунктуации.
- Замена ключевых сущностей (Active Directory → `active_directory`, Cobalt Strike → `cobalt_strike`).
- Комбинированный список стоп-слов (русский + английский) с возможностью расширения файлами и параметрами CLI.
- Опциональная лемматизация русского текста через `pymorphy2`, если библиотека установлена.
- Настраиваемые параметры `min_df`, `max_df`, длина n-грамм и агрегирование TF-IDF (mean/max/sum).
- Фильтрация коротких/числовых токенов и IoC-подобных строк (IP, хэши, домены).
- Вывод топ-N терминов в консоль и/или CSV.

## Установка

```bash
cd tf-idf
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
# при необходимости добавить pymorphy2 для лучшей лемматизации русского
pip install pymorphy2
```

## Быстрый старт

1. Подготовьте папку с `.txt` файлами (каждый файл — отдельный документ корпуса). Пример в `sample_corpus/`.
2. Запустите скрипт:

```bash
python extract_tags.py sample_corpus --top 20 --min-df 1 --max-df 0.8 --max-ngram 2 \
  --extra-stop-words система пользователь позволяет используется
```

3. Сохранить результат в CSV:

```bash
python extract_tags.py sample_corpus --top 100 --output-csv tags.csv
```

## Аргументы CLI

- `source` — путь к каталогу с `.txt` или к одному `.txt` файлу.
- `--top` — сколько терминов показать (по умолчанию 200).
- `--min-df` — минимальное число документов, в которых встречается термин.
- `--max-df` — доля документов (0–1), выше которой термин считается слишком общим.
- `--max-ngram` — максимальная длина n-грамм (1 = униграммы, 2 = уни+би и т.д.).
- `--scoring` — способ агрегирования TF-IDF по корпусу: `mean`, `max` или `sum`.
- `--extra-stop-words` — дополнительные стоп-слова через пробел.
- `--stop-words-file` — путь к файлу со стоп-словами (по одному в строке).
- `--min-length` — минимальная длина термина после фильтрации.
- `--output-csv` — путь для сохранения результата в CSV.

## Замена сущностей

До токенизации выполняются замены, чтобы сохранить важные составные сущности:

- Active Directory → `active_directory`
- Windows Defender → `windows_defender`
- Cobalt Strike → `cobalt_strike`
- Mimikatz → `mimikatz`

Словарь можно расширить в функции `build_default_replacements` внутри `extract_tags.py`.

## Фильтрация мусора

После вычисления TF-IDF термины отбрасываются, если:

- короче `--min-length` символов;
- состоят более чем наполовину из цифр;
- выглядят как IP-адрес, домен или хэш (простые регулярки).

## Пример результата

Для корпуса `sample_corpus` и параметров `--top 10 --min-df 1 --max-df 0.9` вы получите набор терминов вроде:

```
edr_bypass	0.327...
active_directory	0.275...
privilege_escalation	0.201...
```

(значения зависят от выбранной схемы агрегации).
